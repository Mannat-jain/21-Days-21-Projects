# 📚 21 Projects in 21 Days – ML, Deep Learning & GenAI

Welcome to my 21 Days, 21 Projects journey with GeeksforGeeks!

This repo is a daily log of the projects I’ll be building — covering Machine Learning, Deep Learning, and Generative AI.
________________________________________________________________________________________________________________________________
The goal?

🚀 Build consistency

🧠 Strengthen hands-on skills

🎯 Learn by doing
________________________________________________________________________________________________________________________________

📅 Challenge Overview

Duration: 21 Days

Projects: 21 end-to-end projects

Focus Areas: Machine Learning, Deep Learning, Generative AI

Daily Routine: Live class → implement project → push to GitHub
________________________________________________________________________________________________________________________________

🛠️ Tech Stack

Languages: Python

Core: NumPy, Pandas, Scikit-learn

Deep Learning: TensorFlow / PyTorch

GenAI: Hugging Face, OpenAI, LangChain (where applicable)

Other Tools: Streamlit, Flask, Jupyter
________________________________________________________________________________________________________________________________

🌟 Key Learnings (to be updated daily)

📌 Day 1 – Titanic Dataset 🛳️
- Covered: EDA, cleaning, feature engineering, correlation analysis  
- Key Learning: Choosing the right plots for the right analysis makes insights clearer.
  
📌 Day 2 – Netflix Dataset 🍿
- Covered: Data cleaning & transformation, time-series analysis, text data manipulation, geographical & rating analysis, feature engineering, advanced visualization
- Key Learning: EDA isn’t just about numbers — the real value comes from turning raw data into stories that highlight trends, shifts, and patterns clearly.

📌 Day 3 – House Price Prediction (Regression) 🏠
- Covered: Data import via Kaggle API, preprocessing, feature engineering, categorical encoding, Linear Regression vs. XGBoost, model evaluation.
- Key Learning: Regression demands careful handling of targets & features, while the Kaggle API made workflows cleaner and reproducible.

📌 Day 4 – Sentiment Analysis (NLP) 💬
- Covered: Text preprocessing (stopwords, stemming/lemmatization), vectorization (BoW & TF-IDF), ML models for sentiment classification, evaluation (precision/recall/F1).
- Key Learning: In NLP, strong preprocessing + representation often matter more than the choice of model.

📌 Day 5 – Customer Segmentation (Clustering) 🛍️
- Covered: 2D/3D EDA, k-means (income & age-based), optimal k via Elbow Method, hierarchical clustering validation, persona creation for marketing.
- Key Learning: Clustering depends heavily on feature choice + validation — the right setup makes results business-ready.

📌 Day 6 – Predicting Future Store Sales (Time Series) 🏪📈
- Covered: Time series decomposition, stationarity testing (ADF), log transform & differencing, ACF/PACF for parameter selection, ARIMA baseline, SARIMA for trend + seasonality, RMSE evaluation.
- Key Learning: Stationarity is the backbone of forecasting — SARIMA proved far superior by modeling seasonal patterns effectively.
________________________________________________________________________________________________________________________________

📌 Notes

This repo isn’t just about code dumps — I’ll also be documenting learnings, challenges, and takeaways for each project.

Discipline > Perfection ✨
________________________________________________________________________________________________________________________________

🔗 Connect

Follow my journey here:

💼 LinkedIn: https://www.linkedin.com/in/mannatjain14/

📂 GeeksforGeeks Challenge: https://github.com/Mannat-jain/GFG160 and https://github.com/Mannat-jain/21-Days-21-Projects
